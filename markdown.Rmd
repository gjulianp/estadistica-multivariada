---
title: "markdown"
output: html_document
date: "`r Sys.Date()`"
---

# PRIMMERA PREGUNTA

Dadas las siguientes mediciones en 3 variables:
```{r , include=FALSE}
tabla <- read.csv("/home/julian/Documentos/Estadistica multivariada/tabla_0.csv",header = TRUE, row.names = 1)
tabla
```

## A)
Describa el entorno multivariado de estos datos. Encuentre los vectores y matrices X, Sn, R con las fórmulas vistas en clase, descríbalos y corrobórelos.

El código para generar las matrices $\bar{X}$ es el siguiente:

```{r setup, include=TRUE}
knitr::opts_chunk$set(echo = TRUE)


X1 <- as.numeric(tabla[1,])
X2 <- as.numeric(tabla[2,])
X3 <- as.numeric(tabla[3,])

X1 <- mean(X1, na.rm = TRUE)
X2 <- mean(X2, na.rm = TRUE)
X3 <- mean(X3, na.rm = TRUE)

X1
X2
X3

```

- La matriz $\bar{X_{1}}$ = 6.142857
- La matriz $\bar{X_{2}}$ = 8.142857
- La matriz $\bar{X_{3}}$ = 3.428571

La matriz $\bar{S_{n}}$ o matriz de varianza-covarianza, se calcula en R de la siguiente manera:

```{r ,include=TRUE}
Sn <-var(t(tabla)) # matriz de covarianza o matriz de disersion (Sn)
Sn
```

Como se observa en la matriz $\bar{S_{n}}$ , el vector que presenta mayor varianza es el $X_{2}$  con un valor de 13.36263736 y los vectores que tienen mayor covarianza son el $X_{1}$ y $X_{3}$ .

Para la matriz R:

```{r,include=TRUE}
R <- cor(t(tabla)) # Matriz de correlación
R
```
Como se puede observar en la matriz de correlación, existe una correlacion negativa para los vectores $X_{1}$ y $X_{2}$ , las demas relaciones de los vectores son postivas. Esta intepretación se puede observar mejor en la siguiente gráfica:

```{r , echo=FALSE}
tabla_traspuesta <- t(tabla)
tabla_traspuesta <- as.data.frame(tabla_traspuesta)

# Graficar las columnas
plot(tabla_traspuesta)
```

## B) 
Haga una gráfica tipo r o de correlación. en R e interpretelo.

R// La grafica se realiza con el siguiente codigo:

```{r, echo=TRUE}
library(psych)# con una función:
scatter.hist(tabla_traspuesta$X1,tabla_traspuesta$X2,smooth=F,ab=T,density=T,ellipse=F,title="X1 vs X2", xlab="X1", ylab="X2",
             correl=TRUE,pch=20,cex.axis=1.2,cex.lab=1.2,cex.main=1.5)

scatter.hist(tabla_traspuesta$X1,tabla_traspuesta$X3,smooth=F,ab=T,density=T,ellipse=F,title="X1 vs X2", xlab="X1", ylab="X2",
             correl=TRUE,pch=20,cex.axis=1.2,cex.lab=1.2,cex.main=1.5)

scatter.hist(tabla_traspuesta$X2,tabla_traspuesta$X3,smooth=F,ab=T,density=T,ellipse=F,title="X1 vs X2", xlab="X1", ylab="X2",
             correl=TRUE,pch=20,cex.axis=1.2,cex.lab=1.2,cex.main=1.5)
```

Como se logra observar no existe correlación alguna para $X_{1}$ y $X_{2}$.

Para $X_{1}$ y $X_{3}$ si existe correlación ya que se logra ver una pendiente positiva con un r=0.75 

Para $X_{2}$ y $X_{3}$ existe una correlación positiva, que aunque baja con un r=0.28:

## c) 
Con las observaciones, 3, 5 y 12, haga una gráfica tipo q e, interprétela y lo mismo con observaciones, 1, 10 y 13. Si puede relacione ambos casos.

R// Las lineas de codigo para realizar las graficas de 3,5 y 12 son las siguientes:

```{r, include=TRUE}
scatter.hist(tabla$X3,tabla$X5,smooth=F,ab=T,density=T,ellipse=F, xlab="X3", ylab="X5",
             correl=TRUE,pch=20,cex.axis=1.2,cex.lab=1.2,cex.main=1.5)
scatter.hist(tabla$X3,tabla$X12,smooth=F,ab=T,density=T,ellipse=F, xlab="X3", ylab="X12",
             correl=TRUE,pch=20,cex.axis=1.2,cex.lab=1.2,cex.main=1.5)
scatter.hist(tabla$X5,tabla$X12,smooth=F,ab=T,density=T,ellipse=F, xlab="X5", ylab="X12",
             correl=TRUE,pch=20,cex.axis=1.2,cex.lab=1.2,cex.main=1.5)
```

Existe una correlación positiva para las tres observaciones.

En cuanto a las observaciones 1, 10 13:

```{r, include=TRUE}
scatter.hist(tabla$X1,tabla$X10,smooth=F,ab=T,density=T,ellipse=F, xlab="X1", ylab="X10",
             correl=TRUE,pch=20,cex.axis=1.2,cex.lab=1.2,cex.main=1.5)
scatter.hist(tabla$X3,tabla$X12,smooth=F,ab=T,density=T,ellipse=F, xlab="X1", ylab="X13",
             correl=TRUE,pch=20,cex.axis=1.2,cex.lab=1.2,cex.main=1.5)
scatter.hist(tabla$X5,tabla$X12,smooth=F,ab=T,density=T,ellipse=F, xlab="X10", ylab="X13",
             correl=TRUE,pch=20,cex.axis=1.2,cex.lab=1.2,cex.main=1.5)
```

También existen correlación positiva para las observaciones 1, 10 y 13.

## D) 
Por medio de distancias euclidianas intuya como se agruparían estos 6 objetos.

R// Utilizando la distancia euclidiana y la siguiente formula:

El siguiente procedimiento es para hallar la distancia entre los vectores de las observaciones X1 y X3.

$$
\begin{align*}
d(O,P) &= \sqrt{X_{1}^{2}+X_{2}^{2}...} \\
d(X1,X3) &=  \sqrt{(X_{1}-X_{3})²} \\
d(X1,X3) &=  \sqrt{(6-3)²+(8-7)²+(0-2)²} \\
d(X1,X3) &=  \sqrt{(3)²+(1)²+(-2)²} \\
d(X1,X3) &=  \sqrt{(9+1+4} \\
d(X1,X3) &=  \sqrt{14} \\
d(X1,X3) & \approx 3.7417
\end{align*}
$$Para hacer estos calculos con R es de la siguiente manera:

```{r, include=TRUE}
# Crear la matriz de observaciones
observaciones <- matrix(c(tabla$X1, tabla$X3, tabla$X5, 
                          tabla$X7, tabla$X10, tabla$X13
), 
ncol = 3, byrow = TRUE)

# Nombres de las observaciones
nombres_observaciones <- c("X1", "X3", "X5", "X7", "X10", "X13")

# Calcular la matriz de distancias euclidianas
matriz_distancias <- dist(observaciones, method = "euclidean")

# Convertir la matriz de distancias en una matriz regular
matriz_distancias <- as.matrix(matriz_distancias)

# Asignar nombres de observaciones a filas y columnas
rownames(matriz_distancias) <- nombres_observaciones
colnames(matriz_distancias) <- nombres_observaciones

# Mostrar la matriz de distancias con nombres de filas y columnas
matriz_distancias
```
Se puede observar que la observación 1 y 3 tienen una distancia euclidiana menor por lo que estas dos observaciones se podrían agrupar.

## E) 
Por medio de distancias estadísticas intuya como se agruparían estos 7
objetos

R// De la misma manera como se realizo el punto anterior pero dividiendo cada observación sobre la varianza para estandarizar las variables, siendo asi se podrian agrupar las observaciones 1 y 3, 1 y 10 y 5 con 7.


# SEGUNDA PREGUNTA
Con las variables anteriores considere las siguientes combinaciones lineales
3$X_{1}$+$X_{2}$-2$X_{3}$  y $X_{1}$ +2$X_{2}$ +$X_{3}$  

## A)
encuentre su estadísticas conjuntamente.

R//Las lineas de codigo son las siguientes:

```{r,include=TRUE}
# combinaciones lineales

combinacion1 <- 3 * tabla_traspuesta$X1 + tabla_traspuesta$X2 - 2 * tabla_traspuesta$X3
combinacion2 <- tabla_traspuesta$X1 + 2 * tabla_traspuesta$X2 + tabla_traspuesta$X3

# se crea la tabla con los resultados
estadisticas_conjuntas <- data.frame(Combinacion1 = combinacion1,
                                     Combinacion2 = combinacion2)
View(estadisticas_conjuntas)
#  estadisticas descriptivas
summary(estadisticas_conjuntas)
```
Como se puede observar para las dos combinaciones el minimo es 12, sin embargo la combinación 1 tiene una media menor de 19.71 vs 25.86, en cuanto al maximo valor la combinación 2 tiene la mayor cantidad de 41.

# TERCERA PREGUNTA

Considere el conjunto de puntos $X_{1}$ $X_{2}$  cuyas distancias cuadradas al origen están dadas por $C^2$ = 7$X_{1}^2$+3$X_{2}^2$+6$X_{1}$$X_{2}$ para  $C^2$ = 1 y  $C^2$ = 2  .

## A)
Determine los ejes mayor y menor de las elipses para estas distancias y sus longitudes asociadas y analice que sucede en ambos casos.

R// Entiendo que la ecuación representa la siguiente A:

$[ \mathbf{A} = \begin{bmatrix} 7 & 3 \\ 3 & 3 \end{bmatrix} ]$

Para determinar los ejes mayor y menor de las elipses, necesitamos encontrar los ejes principales de la elipse, que son los eigenvectores de la matriz A. Los eigenvectores de A nos darán la dirección de los ejes principales, y los eigenvalores asociados nos darán las longitudes de los semiejes, para hacer este procedimiento con R seria de la siguiente manera:

```{r,include=TRUE}
A <- matrix(c(7, 3, 3, 3), nrow = 2)

# Calculamos los eigenvalores y eigenvectores
eig <- eigen(A)

# Eigenvalores
lambda1 <- eig$values[1]
lambda2 <- eig$values[2]

# Eigenvectores
v1 <- eig$vectors[, 1]
v2 <- eig$vectors[, 2]

# Mostramos los resultados
print("Eigenvalores:")
lambda1
lambda2

print("Eigenvector 1:")
v1

print("Eigenvector 2:")
v2
```
Sabiendo que los eigenvalores son: 8.605551  y 1.394449, por lo que la longitud de los eje son la raiz cuadrada de esos valores: 2.93 y 1.18

# CUARTA PREGUNTA

Evalúe (no tiene relación con el punto anterior):
## A) 
la distancia del punto P(-1, -1) al punto Q(1, 0) usando la distancia
euclidiana con p = 2 y la distancia estadística con

11 12 22
1 1 4
, ,
3 9 27
a a a = = =
## B) 
Dibuje el lugar geométrico de todos los puntos que estén a una distancia
cuadrada = 1 desde Q.

# QUINTA PREGUNTA
Sean los siguientes pares de mediciones en las variables $X_{1}$  y $X_{2}$ :
```{r,include=TRUE}
tabla1 <- read.csv("/home/julian/Documentos/Estadistica multivariada/tabla_1.csv",header = TRUE, row.names = 1)
```

## A)
Grafique los datos como un diagrama de puntos y calcule $S_{11}$ , $S_{22}$ , y $S_{21}$ 

Con el siguiente código se realizó la graficación del diagrama de dispersión:

```{r, include=TRUE}
transpuesta1 <- t(tabla1)
transpuesta1 = data.frame(transpuesta1)
plot(transpuesta1,pch=16)
plot(transpuesta1$X1,pch=16,col="blue",ylab = "X1")
plot(transpuesta1$X2,pch=16,col="red",ylab = "X2")
```
Y para hallar $S_{11}$ , $S_{22}$ , y $S_{21}$ se encuentra con el siguiente codigo:

```{r,include=TRUE}
Sii <-var(t(tabla1)) # matriz de covarianza o matriz de disersion (Sn)
Sii
```
## B)
Calcule los valores en las variables $\tilde{X_{1}}$ y $\tilde{X_{2}}$ asumiendo un ángulo de 30° y calcule
$\tilde{S_{11}}$ , $\tilde{S_{22}}$  y $\tilde{S_{12}}$  Compare con a)

R// Para hallar $\tilde{X_{1}}$ y $\tilde{X_{2}}$ se tiene en cuenta la siguiente formula:

$$
\tilde{X_{i}} = {X_{i}}Cos(\theta) + {X_{i}}Sen(\theta) 
$$
Y con R:
```{r,include=TRUE}
theta = 30
tilde_X1 <- transpuesta1$X1 * cos(theta) + transpuesta1$X1 * sin(theta)
tilde_X2 <- -transpuesta1$X2 * sin(theta) + transpuesta1$X2 * cos(theta)
tilde_X1
tilde_X2
```
Comparando los resultados con R los vectores arrojaron como resultados valores negativos muy distintos a los originales

Para hallar  $\tilde{S_{11}}$ , $\tilde{S_{22}}$ y $\tilde{S_{12}}$:
```{r,include=TRUE}
data_tildes <- data.frame(tilde_X1 = tilde_X1, tilde_X2 = tilde_X2)

tildeSii <-var(data_tildes) # matriz de covarianza o matriz de disersion (Sn)
tildeSii
```
## C) 

Considere un nuevo par (${X_{1}}$ y ${X_{2}}$ ) = (4, -2), transfórmelo a $\tilde{X_{1}}$ y $\tilde{X_{2}}$ 
y calcule la distancia d(O, P) del nuevo punto P( $\tilde{X_{1}}$ , $\tilde{X_{2}}$ ) desde el origen.

R// Entendiendo que:

$$
d (O,P) =  \sqrt{{a_{11}}{x_{1}²}+2{a_{12}}{x_{1}}{x_{2}}+{a_{22}}{x_{2}²}}
$$
$$
{a_{11}}=\frac{Cos^2{\theta}}{Cos^2{\theta}s_{11}+2Sen{\theta}Cos{\theta}s_{12}+Sen^2{\theta}s_{22}}+\frac{Sen^2{\theta}}{Cos^2{\theta}s_{22}-2Sen{\theta}Cos{\theta}s_{12}+Sen^2{\theta}s_{11}}
$$

$$
{a_{22}}=\frac{Sen^2{\theta}}{Cos^2{\theta}s_{11}+2Sen{\theta}Cos{\theta}s_{12}+Sen^2{\theta}s_{22}}+\frac{Cos^2{\theta}}{Cos^2{\theta}s_{22}-2Sen{\theta}Cos{\theta}s_{12}+Sen^2{\theta}s_{11}}
$$

$$
{a_{12}}=\frac{Cos{\theta}Sen{\theta}}{Cos^2{\theta}s_{11}+2Sen{\theta}Cos{\theta}s_{12}+Sen^2{\theta}s_{22}}-\frac{Sen{\theta}Cen{\theta}}{Cos^2{\theta}s_{22}-2Sen{\theta}Cos{\theta}s_{12}+Sen^2{\theta}s_{11}}
$$

Utilizando estas formulas con R, se obtiene:
```{r,include=TRUE}
# Definir los valores de X1 y X2
x1 <- 4
x2 <- -2

# Definir el ángulo theta en grados
theta_grados <- 30

# Calcular la transformación
tildeX1 <- x1 * cos(theta_grados) + x2 * sin(theta_grados)
tildeX2 <- x1 * -sin(theta_grados) + x2 * cos(theta_grados)

# Crear el DataFrame con los resultados
resultados <- data.frame(tilde_X1 = tilde_X1, tilde_X2 = tilde_X2)
resultadosii <- var(resultados)
# Imprimir el DataFrame
resultadosii
```

## D)
```{r,include=TRUE}
# Definir los valores de s11, s12 y s22
s11 <- 3.972511
s12 <- 1.428619
s22 <- 18.989655

# Definir el ángulo theta en grados
theta_grados <- 30

# Calcular los términos de la fórmula
cos_theta <- cos(theta_grados)
sin_theta <- sin(theta_grados)

# Calcular a11 según la fórmula dada
a11 <- (cos_theta^2) / ((cos_theta^2 * s11) + (2 * sin_theta * cos_theta * s12) + (sin_theta^2 * s22)) +
  (sin_theta^2) / ((cos_theta^2 * s22) - (2 * sin_theta * cos_theta * s12) + (sin_theta^2 * s11))

# Imprimir el resultado
print(a11)

# Calcular a22 según la fórmula dada
a22 <- (sin_theta^2) / ((cos_theta^2 * s11) + (2 * sin_theta * cos_theta * s12) + (sin_theta^2 * s22)) +
  (cos_theta^2) / ((cos_theta^2 * s22) - (2 * sin_theta * cos_theta * s12) + (sin_theta^2 * s11))

# Imprimir el resultado
print(a22)

# Calcular a12 según la fórmula dada
a12 <- (cos_theta * sin_theta) / ((cos_theta^2 * s11) + (2 * sin_theta * cos_theta * s12) + (sin_theta^2 * s22)) -
  (sin_theta * cos_theta) / ((cos_theta^2 * s22) - (2 * sin_theta * cos_theta * s12) + (sin_theta^2 * s11))

# Imprimir el resultado
print(a12)

# Calcular la distancia d(O,P) según la fórmula dada
distancia <- sqrt(a11 * x1^2 + 2 * a12 * x1 * x2 + a22 * x2^2)

# Imprimir el resultado
print(distancia)
```

# SEXTA PREGUNTA

Dados los vectores X' = [2 3 1] y Y' = [3 1 -3]

## A)
Determine las longitudes de ellos
R// Utilizando el código:

```{r,include=TRUE}
Xtranspueto <- c(2,3,1)
Ytranspuesto <- c(3,1,-3)

# a) Longitud de los vectores
longitud_Xtranspuesto <- sqrt(sum(Xtranspueto^2))
longitud_Ytranspuesto <- sqrt(sum(Ytranspuesto^2))
```
## B)
vectores sombra

R// Según la formula:

$$
Vs = \frac{X'Y}{Y'Y}Y
$$
```{r,include=TRUE}
Xnormal <- t(Xtranspueto)
Ynormal <- t(Ytranspuesto)
Vs = ((Xtranspueto*Ynormal)/(Ytranspuesto*Ynormal))*Ynormal
print(Vs)
```
## c)
Vuelva los vectores de a) de longitud unitaria

R// 

```{r,include=TRUE}
XLu = Xnormal/(longitud_Xtranspuesto)
YLu = Ynormal/(longitud_Ytranspuesto)
print(XLu)
print(YLu)
```

# SEPTIMA PREGUNTA
Diga por dos métodos diferentes si los siguientes vectores:

X1 = [9, 7, 3] X2 =[4,10,7] y X3 = [5,7,2]

## A) 
Son linealmente independientes:

9C1 + 4C2 + 5C3 = 0
7C1 + 10C2 + 7C3 = 0
3C3 + 7C2 + 2C3 = 0

- Haciendo el calculo manual se encuentra que C1, C2 y C3 no se pueden encontrar 3 constantes diferentes a 0, por lo tanto son linealmente independientes.
- Otro metodo para resolver este sistema de ecuaciones lineales es:

```{r,include=TRUE}
Xmatriz <- matrix(c(9,7,3,4,10,7,5,7,2),nrow =3)
Bmatriz <- matrix(c(0,0,0),nrow=3)
solve(Xmatriz,Bmatriz)
```
El método solve en R arroja resultados en la consola, por lo que es linealmente independiente.

## B)
Encuentre “usando inversiones y explicándolas”, el determinante de la
matriz A formada con ellos.

R// Utilizando inversiones el determinante de la matriz A seria:

9⋅10⋅2−9⋅7⋅7−4⋅7⋅2+4⋅7⋅3+5⋅7⋅7−5⋅10⋅3=−138

## C)
Corrobore sus resultados usando R
```{r,include=TRUE}
det(Xmatriz)
```

# OCTAVA PREGUNTA

## A)

Usando la descomposición espectral encuentre la inversa de la matriz obtenida en el punto 3.

## B)
Encuentre A^2 y verifique sus resultados
## C)
Encuentre la inversa de A en R.

R//

Calculado estos resultados a mano seria:

$A^{-1} = Q \Lambda^{-1} Q^{-1}$ Donde: $Q = \begin{bmatrix} v_1 & v_2 \end{bmatrix}$ es la matriz cuyas columnas son los eigenvectores de \( A \),

$\Lambda = \begin{bmatrix} \lambda_1 & 0 \\ 0 & \lambda_2 \end{bmatrix}$ es una matriz diagonal que contiene los eigenvalores de \( A \),

$\Lambda^{-1} = \begin{bmatrix} \frac{1}{\lambda_1} & 0 \\ 0 & \frac{1}{\lambda_2} \end{bmatrix}$ es una matriz diagonal que contiene los inversos de los eigenvalores de A

$Q^{-1} = Q^T$ es la inversa de la matriz \( Q \). Entonces, el cálculo de la inversa de \( A \) se expresa como:

$A^{-1} = \begin{bmatrix} v_1 & v_2 \end{bmatrix} \begin{bmatrix} \frac{1}{\lambda_1} & 0 \\ 0 & \frac{1}{\lambda_2} \end{bmatrix} \begin{bmatrix} v_1^T \\ v_2^T \end{bmatrix}$ Esta expresión representa el cálculo de la inversa de $A$ utilizando la descomposición espectral.

Para calcular $A^2$, simplemente multiplicamos la matriz \( A \) por sí misma:

$A^2 = A \times A$


y utilizando el siguiente codigo se calcula la matriz inversa, mediante la descomposición espectral:

```{r,include=TRUE}
# Calculamos la inversa de los eigenvalores
inv_lambda1 <- 1 / lambda1
inv_lambda2 <- 1 / lambda2

# Calculamos la matriz diagonal de los inversos de los eigenvalores
inv_Lambda <- diag(c(inv_lambda1, inv_lambda2))

# Calculamos la matriz Q^-1
Q_inv <- solve(eig$vectors)

# Calculamos la inversa de A
A_inv <- Q_inv %*% inv_Lambda %*% t(Q_inv)

# Mostramos la inversa de A
print("Inversa de A:")
print(A_inv)
```

# NOVENA PREGUNTA
9. Para las 2 variables aleatorias $X_1$: Número de apariciones de una especie vegetal y $X_2$: Aparición de unas enredaderas asociadas, que al muestrear 100 parcelas conjuntamente presentaron los siguientes datos:

R//

```{r,include=TRUE}
#A)
num_apar_especie <- c(1,1,1,2,2,2,3,3,3,4,4,4)
num_apar_enre <- c(1,2,3,1,2,3,1,2,3,1,2,3)
frecuencias <- c(12,5,15,15,6,2,7,2,2,26,7,1)

# Creación del dataframe
df9 <- data.frame(num_apar_especie, num_apar_enre, frecuencias)

# Encontrar la matriz de covarianzas
cov_matrix9 <- cov(df9)
print(cov_matrix9)

#B)
# Encontrar la matriz de correlaciones
cor_matrix9 <- cor(df9)
print(cor_matrix9)

#C)
# Calcular los eigenvalores de la matriz de covarianzas
eigen_values <- eigen(cov_matrix9)$values

# Verificar si todos los eigenvalores son positivos
is_positive_definite <- all(eigen_values > 0)

if (is_positive_definite) {
  print("La matriz de covarianzas es definida positiva.")
} else {
  print("La matriz de covarianzas NO es definida positiva.")
}
# Calcular los valores y vectores propios de la matriz de covarianzas
eigen_info <- eigen(cov_matrix9)

#D)
# Valores propios
eigenvalues <- eigen_info$values

# Vectores propios
eigenvectors <- eigen_info$vectors

print("Valores propios:")
print(eigenvalues)

print("Vectores propios:")
print(eigenvectors)
# Graficar los eigenvalores
plot(eigen_values, type = 'b', main = "Eigenvalores de la matriz de covarianzas", 
     xlab = "Índice", ylab = "Eigenvalor")
```


10. La Tabla siguiente contiene 13 variables ambientales sobre contaminación del
aire en el año 2007 para 41 municipios de carácter industrial menor.
```{r}
tabla10 <- read.csv("/home/julian/Documentos/Estadistica multivariada/tabla_3.csv")
```

a) Forme grupos de variables y exprese el porqué de su decisión

R// La variables se pueden agrupar a simple observación de la siguiente manera:

- Variables ambientales: Polución, Temperatura promedio, Velocidad del viento, Lluvia, Días de lluvia y Radiación solar, al relacionarse por aspectos ambientales y climaticos, pueden ir ligadas entre si.



- Variables socioeconómicas: Industrias, Población, CO, NO, NO2, O3, HC, se agrupan estas ya que el aumento de las industrias y población es directamente proporcional a la concentracion de estos gases de efecto invernadero según la literatura.


R// la pregunta 10 se resuelve de la siguiente manera:

```{r,include=TRUE}
# Cargar la biblioteca ggplot2
library(ggplot2)

# Diagrama marginal para Uradsol (Radiación solar) y CO
scatter_plot_uradsol <- ggplot(tabla10, aes(x = Uradsol, y = CO)) +
  geom_point() + 
  labs(x = "Radiación solar (Uradsol)", y = "Monóxido de carbono (CO)",
       title = "Diagrama marginal: Radiación solar vs. Monóxido de carbono")

# Diagrama marginal para Viento y Industrias
scatter_plot_viento <- ggplot(tabla10, aes(x = Velocvient, y = Industrias)) +
  geom_point() +
  labs(x = "Industrias", y = "Velocidad del viento",
       title = "Diagrama marginal: Velocidad del viento vs industrias")

# Mostrar los gráficos
scatter_plot_uradsol
scatter_plot_viento

#b)
# Calcular el vector de medias
medias <- colMeans(tabla10[-1])
# Calcular la matriz de covarianza
cov_matriz <- cov(tabla10[,-1])

# Calcular la matriz de correlación
cor_matriz <- cor(tabla10[,-1])


print(medias)
print(cov_matriz)
print(cor_matriz)

library(corrplot)
corrplot(cor_matriz, method = "square",tl.col = "black")
# D)
# Descomposición espectral de la matriz de covarianza Sn
eigen_Sn <- eigen(cov_matriz)

# Descomposición espectral de la matriz de correlación R
eigen_R <- eigen(cor_matriz)

# Calcular Sn^2 y R^3 después de la descomposición espectral
Sn_cuadrado <- eigen_Sn$vectors %*% diag(eigen_Sn$values^2) %*% t(eigen_Sn$vectors)
R_cubo <- eigen_R$vectors %*% diag(eigen_R$values^3) %*% t(eigen_R$vectors)

#E)
# Transponer la tabla
tabla_transpuesta <- t(tabla10)

# Establecer la primera fila como nombres de columna
colnames(tabla_transpuesta) <- tabla_transpuesta[1, ]

# Eliminar la primera fila
tabla_transpuesta <- tabla_transpuesta[-1, ]

# Eliminar el encabezado de las filas
rownames(tabla_transpuesta) <- NULL

# Convertir a dataframe
df <- as.data.frame(tabla_transpuesta, stringsAsFactors = FALSE)

# Convertir cada columna a numérica
for (i in 1:ncol(df)) {
  df[, i] <- as.numeric(df[, i])
}

# Crear un nuevo dataframe con solo las primeras tres filas
df_primeras_tres <- df[1:3, ]

# Calcular la distancia euclidiana entre las filas
distancias <- dist(df_primeras_tres, method = "euclidean")

# Calcular la matriz de distancias entre columnas
distancias_columnas <- dist(t(df_primeras_tres), method = "euclidean")

# Realizar agrupamiento jerárquico aglomerativo
agrupamiento <- hclust(distancias_columnas, method = "complete")

# Obtener los grupos
grupos <- cutree(agrupamiento, k = 2)  
# Mostrar la asignación de columnas a grupos
print(grupos)

# Graficar el dendrograma con parámetros ajustados
plot(agrupamiento, hang = -1, main = "Dendrograma de Agrupamiento de Columnas", xlab = "Columnas", ylab = "Altura de Fusión", cex = 0.8)

# F

promedio_variables <- colMeans(df_primeras_tres)

# Tipificando los datos
tipified_data <- scale(df_primeras_tres)
```
```{r,include=FALSE,echo=FALSE}
# Calcular la distancia euclidiana entre cada columna y el vector de medias tipificadas
distancias <- apply(tipified_data, 2, function(col) sqrt(sum((col - promedio_variables)^2)))
```
```{r,include=TRUE}
# Encontrar la columna más alejada
columna_mas_alejada <- which.max(distancias)

print(distancias)
# Mostrar el índice de la columna más alejada
print(columna_mas_alejada)
```